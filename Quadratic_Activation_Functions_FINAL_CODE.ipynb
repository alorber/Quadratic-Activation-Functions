{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE472_Project_Code_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOD6AfBguyppMAjuj2llCS0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alorber/Quadratic-Activation-Functions/blob/main/Quadratic_Activation_Functions_FINAL_CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RvEK6yqdXb"
      },
      "source": [
        "# **Quadratic Neurons as Activation Functions in Deep Neural Networks**\n",
        "## Andrew Lorber & Mark Koszykowski\n",
        "### Based on *Pattern Classification using Quadratic Neuron: An Experimental Study* by Ganesh, Singh, and Murthy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mnr0BlLqWhh"
      },
      "source": [
        "# Imports\n",
        "import tensorflow as tf\n",
        "import tensorflow.math as tfm\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Layer, InputLayer, Flatten, Dense, Conv2D, MaxPool2D, BatchNormalization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MevvJwTEqqpH"
      },
      "source": [
        "## Quadratic Neuron Layer Classes\n",
        "Implementations of the Quadratic Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CB66BNGqvzU"
      },
      "source": [
        "### Quadratic Neuron Layer\n",
        "### Layer-Wide Coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyORDoRfq9KX"
      },
      "source": [
        "class Quadratic(Layer):\n",
        "    def __init__(self, trainable=True, **kwargs):\n",
        "        super(Quadratic, self).__init__(**kwargs)\n",
        "        self.trainable = trainable\n",
        "        # Initializes Coefficients\n",
        "        self.coeffs = self.add_weight(shape=(1,6), initializer='random_normal', trainable=self.trainable)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # Builds input\n",
        "        x, y = tf.split(inputs, num_or_size_splits=2, axis=1)\n",
        "        x2 = tfm.square(x)\n",
        "        y2 = tfm.square(y)\n",
        "        xy = tfm.multiply(x, y)\n",
        "\n",
        "        quad_inputs = tf.stack([x2, xy, y2, x, y, tf.ones((tf.shape(x)))], axis=1)\n",
        "        quad_outputs = tf.squeeze(tf.matmul(self.coeffs, quad_inputs), axis=[1])\n",
        "\n",
        "        return quad_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape/2 "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRe5RiTyrAcV"
      },
      "source": [
        "### Convolutional Quadratic Neuron Layer\n",
        "### Layer-Wide Coefficients\n",
        "Quadratic Activation Function for Convolutional Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lQFmDZAs7C8"
      },
      "source": [
        "class ConvQuadratic(Layer):\n",
        "    def __init__(self, trainable=True, **kwargs):\n",
        "        super(ConvQuadratic, self).__init__(**kwargs)\n",
        "        self.trainable = trainable\n",
        "        # Initializes Coefficients\n",
        "        self.coeffs = self.add_weight(shape=(1,6), initializer='random_normal', trainable=self.trainable)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # Saves dimensions to make code nicer\n",
        "        batch_size = inputs.shape[0]\n",
        "        height = inputs.shape[1]\n",
        "        width = inputs.shape[2]\n",
        "        num_filters = inputs.shape[3]\n",
        "\n",
        "        # Reshapes so last 2 dimensions are a single filter\n",
        "        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
        "\n",
        "        # Reshapes into columns for x,y\n",
        "        inputs = tf.reshape(inputs, [batch_size, num_filters, -1, 2])\n",
        "\n",
        "        # Transposes to get correct dimensions for matmul\n",
        "        inputs = tf.transpose(inputs, [0,1,3,2])\n",
        "\n",
        "        # Splits tensor into x & y\n",
        "        x, y = tf.split(inputs,2,2)\n",
        "\n",
        "        # Calculates other components of quadratic input\n",
        "        x2 = tfm.square(x)\n",
        "        y2 = tfm.square(y)\n",
        "        xy = tfm.multiply(x, y)\n",
        "\n",
        "        # Builds quadratic input\n",
        "        quad_input = tf.concat([x2, xy, y2, x, y, tf.ones((tf.shape(x)))], axis=2)\n",
        "\n",
        "        # Matmul -> Quadratic output\n",
        "        quad_output = tf.matmul(self.coeffs, quad_input)\n",
        "\n",
        "        # Reshapes back to original size with width / 2\n",
        "        quad_output = tf.reshape(quad_output, [batch_size, num_filters, height, int(width/2)])\n",
        "\n",
        "        # Rearranges axis to have num_filters last (as CONV2D expects)\n",
        "        quad_output = tf.transpose(quad_output, [0,2,3,1])\n",
        "\n",
        "        return quad_output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[2]/2, input_shape[3])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1WIrRbCtAtT"
      },
      "source": [
        "### Quadratic Neuron Layer\n",
        "### Per-Neuron Coefficients\n",
        "Each neuron has its own coefficients (as opposed to layer-wide coefficients)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffqctLlltBWI"
      },
      "source": [
        "class QuadraticPerN(Layer):\n",
        "    def __init__(self, trainable=True, **kwargs):\n",
        "        super(QuadraticPerN, self).__init__(**kwargs)\n",
        "        self.trainable = trainable\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Initializes Coefficients\n",
        "        self.coeffs = self.add_weight(shape=(int(input_shape[1]/2),6), initializer='random_normal', trainable=self.trainable)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # Builds input\n",
        "        x, y = tf.split(inputs, num_or_size_splits=2, axis=1)\n",
        "        x2 = tfm.square(x)\n",
        "        y2 = tfm.square(y)\n",
        "        xy = tfm.multiply(x, y)\n",
        "\n",
        "        quad_inputs = tf.stack([x2, xy, y2, x, y, tf.ones((tf.shape(x)))], axis=1)\n",
        "        quad_outputs = tf.reduce_sum(tf.multiply(self.coeffs, tf.transpose(quad_inputs, perm=(0,2,1))),2)\n",
        "\n",
        "        return quad_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape/2 "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZs2PTYJtFj2"
      },
      "source": [
        "### Convolutional Quadratic Neuron Layer\n",
        "### Per-Neuron Coefficients\n",
        "Quadratic Activation Function for Convolutional Layer\n",
        "\n",
        "Each neuron has its own coefficients (as opposed to layer-wide coefficients)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oms39oC9tGN0"
      },
      "source": [
        "class ConvQuadraticPerN(Layer):\n",
        "    def __init__(self, trainable=True, **kwargs):\n",
        "        super(ConvQuadraticPerN, self).__init__(**kwargs)\n",
        "        self.trainable = trainable\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.coeffs = self.add_weight(shape=(int(input_shape[2]/2) * input_shape[1],6), initializer='random_normal', trainable=self.trainable)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # Saves dimensions to make code nicer\n",
        "        batch_size = inputs.shape[0]\n",
        "        height = inputs.shape[1]\n",
        "        width = inputs.shape[2]\n",
        "        num_filters = inputs.shape[3]\n",
        "\n",
        "        # Reshapes so last 2 dimensions are a single filter\n",
        "        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
        "\n",
        "        # Reshapes into columns for x,y\n",
        "        inputs = tf.reshape(inputs, [batch_size, num_filters, -1, 2])\n",
        "\n",
        "        # Splits tensor into x & y\n",
        "        x, y = tf.split(inputs,2,3)\n",
        "\n",
        "        # Calculates other components of quadratic input\n",
        "        x2 = tfm.square(x)\n",
        "        y2 = tfm.square(y)\n",
        "        xy = tfm.multiply(x, y)\n",
        "\n",
        "        # Builds quadratic input\n",
        "        quad_input = tf.concat([x2, xy, y2, x, y, tf.ones((tf.shape(x)))], axis=3)\n",
        "\n",
        "        # Gets inner product of each neuron and its coefficients\n",
        "        quad_output = tf.reduce_sum(tf.multiply(self.coeffs, quad_input), 3)\n",
        "\n",
        "        # Reshapes back to original size with width / 2\n",
        "        quad_output = tf.reshape(quad_output, [batch_size, num_filters, height, int(width/2)])\n",
        "\n",
        "        # Rearranges axis to have num_filters last (as CONV2D expects)\n",
        "        quad_output = tf.transpose(quad_output, [0,2,3,1])\n",
        "\n",
        "        return quad_output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[2]/2, input_shape[3])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAi09HeZtNHC"
      },
      "source": [
        "## Model Builder Function\n",
        "Builds models with given layer sizes, which are all exactly the same except for the layer activation functions. \n",
        "\n",
        "The activation functions included are: *Sigmoid*, *ReLu*, *Quadratic* (Layer-Wide Coefficients), *Quadratic* \n",
        "(Per-Neuron Coefficients)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCJqhqCbtVkV"
      },
      "source": [
        "# @params: layer_sizes = len is number of hidden layers, value is number of neurons at layer\n",
        "#          layer_types = len is number of hidden layers, value is type of layer\n",
        "#                       Options: Dense, Conv, MaxPool, Flatten\n",
        "#          output_size = number of neurons at output layer\n",
        "def buildModels(layer_sizes, layer_types, output_size = 10):\n",
        "\n",
        "    # Sigmoid Model\n",
        "    # --------------\n",
        "\n",
        "    # Input Layer\n",
        "    sigmoid_model = Sequential()\n",
        "\n",
        "    if layer_types[0] == \"Dense\":\n",
        "        sigmoid_model.add(Flatten())\n",
        "\n",
        "    # Hidden Layers\n",
        "    for (l,t) in zip(layer_sizes,layer_types):\n",
        "        if t == \"Dense\":\n",
        "            sigmoid_model.add(Dense(l, kernel_regularizer='l2', activation=tf.nn.sigmoid))\n",
        "            sigmoid_model.add(BatchNormalization())\n",
        "        elif t == \"Conv\": \n",
        "            sigmoid_model.add(Conv2D(l, 3, padding='same', kernel_regularizer='l2', activation=tf.nn.sigmoid))\n",
        "            sigmoid_model.add(BatchNormalization())\n",
        "        elif t == \"MaxPool\":\n",
        "            sigmoid_model.add(MaxPool2D(l))\n",
        "        elif t == \"Flatten\":\n",
        "            sigmoid_model.add(Flatten())\n",
        "    # Output Layer\n",
        "    sigmoid_model.add(Dense(output_size, kernel_regularizer='l2', activation=tf.nn.softmax))\n",
        "\n",
        "\n",
        "    # ReLu Model\n",
        "    # -----------\n",
        "\n",
        "    # Input Layer\n",
        "    relu_model = Sequential()\n",
        "\n",
        "    if layer_types[0] == \"Dense\":\n",
        "        relu_model.add(Flatten())\n",
        "    \n",
        "    # Hidden Layers\n",
        "    for (l,t) in zip(layer_sizes,layer_types):\n",
        "        if t == \"Dense\":\n",
        "            relu_model.add(Dense(l, kernel_regularizer='l2', activation=tf.nn.relu))\n",
        "            relu_model.add(BatchNormalization())\n",
        "        elif t == \"Conv\": \n",
        "            relu_model.add(Conv2D(l, 3, padding='same', kernel_regularizer='l2', activation=tf.nn.relu))\n",
        "            relu_model.add(BatchNormalization())\n",
        "        elif t == \"MaxPool\":\n",
        "            relu_model.add(MaxPool2D(l))\n",
        "        elif t == \"Flatten\":\n",
        "            relu_model.add(Flatten())\n",
        "\n",
        "    # Output Layer\n",
        "    relu_model.add(Dense(output_size, kernel_regularizer='l2', activation=tf.nn.softmax))\n",
        "\n",
        "\n",
        "    # Quadratic Model (Layer-wide Coefficients)\n",
        "    # ---------------------------------------------\n",
        "\n",
        "    # Input Layer\n",
        "    layer_model = Sequential()\n",
        "\n",
        "    if layer_types[0] == \"Dense\":\n",
        "        layer_model.add(Flatten())\n",
        "    \n",
        "    # Hidden Layers\n",
        "    for (l,t) in zip(layer_sizes,layer_types):\n",
        "        if t == \"Dense\":\n",
        "            layer_model.add(Dense(l, kernel_regularizer='l2', activation=Quadratic()))\n",
        "            layer_model.add(BatchNormalization())\n",
        "        elif t == \"Conv\": \n",
        "            layer_model.add(Conv2D(l, 3, padding='same', kernel_regularizer='l2', activation=ConvQuadratic()))\n",
        "            layer_model.add(BatchNormalization())\n",
        "        elif t == \"MaxPool\":\n",
        "            layer_model.add(MaxPool2D(l))\n",
        "        elif t == \"Flatten\":\n",
        "            layer_model.add(Flatten())\n",
        "    # Output Layer\n",
        "    layer_model.add(Dense(output_size, kernel_regularizer='l2', activation=tf.nn.softmax))\n",
        "\n",
        "\n",
        "    # Quadratic Model (Per-neuron Coefficients)\n",
        "    # ---------------------------------------------\n",
        "\n",
        "    # Input Layer\n",
        "    neuron_model = Sequential()\n",
        "\n",
        "    if layer_types[0] == \"Dense\":\n",
        "        neuron_model.add(Flatten())\n",
        "   \n",
        "    # Hidden Layers\n",
        "    for (l,t) in zip(layer_sizes,layer_types):\n",
        "        if t == \"Dense\":\n",
        "            neuron_model.add(Dense(l, kernel_regularizer='l2', activation=QuadraticPerN()))\n",
        "            neuron_model.add(BatchNormalization())\n",
        "        elif t == \"Conv\": \n",
        "            neuron_model.add(Conv2D(l, 3, padding='same', kernel_regularizer='l2', activation=ConvQuadraticPerN()))\n",
        "            neuron_model.add(BatchNormalization())\n",
        "        elif t == \"MaxPool\":\n",
        "            neuron_model.add(MaxPool2D(l))\n",
        "        elif t == \"Flatten\":\n",
        "            neuron_model.add(Flatten())\n",
        "    # Output Layer\n",
        "    neuron_model.add(Dense(output_size, kernel_regularizer='l2', activation=tf.nn.softmax))\n",
        "\n",
        "\n",
        "    # Prints Model Architecture\n",
        "    for (l,t) in zip(layer_sizes, layer_types):\n",
        "        print(f\"{t} Layer - {l} Neurons\")\n",
        "        \n",
        "    # Returns models\n",
        "    return sigmoid_model, relu_model, layer_model, neuron_model\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}